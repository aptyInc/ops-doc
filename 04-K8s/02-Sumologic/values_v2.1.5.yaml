fluentd:
  image:
    repository: sumologic/kubernetes-fluentd
    tag: 1.1.0
    pullPolicy: IfNotPresent
  additionalPlugins: []
  logLevel: info
  logLevelFilter: true
  verifySsl: true
  proxyUri: ""
  securityContext:
    fsGroup: 999
  persistence:
    enabled: false
    accessMode: ReadWriteOnce
    size: 10Gi
  buffer:
    type: memory
    flushInterval: 5s
    numThreads: 8
    chunkLimitSize: 1m
    queueChunkLimitSize: 128
    totalLimitSize: 128m
    retryMaxInterval: 10m
    retryForever: true
    filePaths:
      logs:
        containers: /fluentd/buffer/logs.containers
        kubelet: /fluentd/buffer/logs.kubelet
        systemd: /fluentd/buffer/logs.systemd
        default: /fluentd/buffer/logs.default
      metrics:
        apiserver: /fluentd/buffer/metrics.apiserver
        kubelet: /fluentd/buffer/metrics.kubelet
        container: /fluentd/buffer/metrics.container
        controller: /fluentd/buffer/metrics.controller
        scheduler: /fluentd/buffer/metrics.scheduler
        state: /fluentd/buffer/metrics.state
        node: /fluentd/buffer/metrics.node
        control-plane: /fluentd/buffer/metrics.control_plane
        default: /fluentd/buffer/metrics.default
      events: /fluentd/buffer/events
      traces: /fluentd/buffer/traces
    extraConf: ""
  monitoring:
    input: false
    output: false
  metadata:
    cacheSize: 10000
    cacheTtl: 3600
    cacheRefresh: 1800
    pluginLogLevel: error
    coreApiVersions:
      - v1
    apiGroups:
      - apps/v1
      - extensions/v1beta1
  logs:
    enabled: true
    statefulset:
      nodeSelector: {}
      tolerations: {}
      affinity: {}
      podAntiAffinity: soft
      replicaCount: 2
      resources:
        limits:
          memory: 1Gi
          cpu: 1
        requests:
          memory: 768Mi
          cpu: 0.5
    autoscaling:
      enabled: false
      minReplicas: 3
      maxReplicas: 10
      targetCPUUtilizationPercentage: 50
    rawConfig: |-
      @include common.conf
      @include logs.conf
    output:
      logFormat: fields
      addTimestamp: true
      timestampKey: timestamp
      pluginLogLevel: error
      extraConf: ""
    extraLogs: ""
    containers:
      overrideRawConfig: ""
      outputConf: '@include logs.output.conf'
      overrideOutputConf: ""
      sourceName: '%{namespace}.%{pod}.%{container}'
      sourceCategory: '%{namespace}/%{pod_name}'
      sourceCategoryPrefix: kubernetes/
      sourceCategoryReplaceDash: /
      excludeContainerRegex: ""
      excludeHostRegex: ""
      excludeNamespaceRegex: ""
      excludePodRegex: ""
      k8sMetadataFilter:
        watch: true
        caFile: ""
        verifySsl: true
        clientCert: ""
        clientKey: ""
        bearerTokenFile: ""
      extraFilterPluginConf: ""
    kubelet:
      enabled: true
      outputConf: '@include logs.output.conf'
      overrideOutputConf: ""
      sourceName: k8s_kubelet
      sourceCategory: kubelet
      sourceCategoryPrefix: kubernetes/
      sourceCategoryReplaceDash: /
      excludeFacilityRegex: ""
      excludeHostRegex: ""
      excludePriorityRegex: ""
      excludeUnitRegex: ""
    systemd:
      enabled: true
      outputConf: '@include logs.output.conf'
      overrideOutputConf: ""
      sourceCategory: system
      sourceCategoryPrefix: kubernetes/
      sourceCategoryReplaceDash: /
      excludeFacilityRegex: ""
      excludeHostRegex: ""
      excludePriorityRegex: ""
      excludeUnitRegex: ""
    default:
      outputConf: '@include logs.output.conf'
      overrideOutputConf: ""
  metrics:
    enabled: false
    statefulset:
      nodeSelector: {}
      tolerations: {}
      affinity: {}
      podAntiAffinity: soft
      replicaCount: 1
      resources:
        limits:
          memory: 1Gi
          cpu: 1
        requests:
          memory: 768Mi
          cpu: 0.5
    autoscaling:
      enabled: false
      minReplicas: 3
      maxReplicas: 10
      targetCPUUtilizationPercentage: 50
    rawConfig: |-
      @include common.conf
      @include metrics.conf
    outputConf: '@include metrics.output.conf'
    extraFilterPluginConf: "<filter **>\n  @type grep\n    <exclude>\n      key namespace\n      pattern /sumologic$/\n    </exclude>\n\n    <exclude>\n      key namespace\n      pattern /kubernetes-dashboard/\n    </exclude>\n\n    <exclude>\n      key namespace\n      pattern /kube-system/\n    </exclude>\n\n    <exclude>\n      key namespace\n      pattern /default/\n    </exclude>\n    <exclude>\n      key namespace\n      pattern /kube-node-lease/\n    </exclude>      \n    <exclude>\n      key namespace\n      pattern /kube-public/\n    </exclude>                  \n</filter>          "
    extraOutputPluginConf: ""
    output:
      apiserver:
        tag: prometheus.metrics.apiserver**
        id: sumologic.endpoint.metrics.apiserver
        weight: 90
      kubelet:
        tag: prometheus.metrics.kubelet**
        id: sumologic.endpoint.metrics.kubelet
        weight: 90
      container:
        tag: prometheus.metrics.container**
        id: sumologic.endpoint.metrics.container
        source: kubelet
        weight: 90
      controller:
        tag: prometheus.metrics.controller-manager**
        id: sumologic.endpoint.metrics.kube.controller.manager
        weight: 90
      scheduler:
        tag: prometheus.metrics.scheduler**
        id: sumologic.endpoint.metrics.kube.scheduler
        weight: 90
      state:
        tag: prometheus.metrics.state**
        id: sumologic.endpoint.metrics.kube.state
        weight: 90
      node:
        tag: prometheus.metrics.node**
        id: sumologic.endpoint.metrics.node.exporter
        weight: 90
      control-plane:
        tag: prometheus.metrics.control-plane**
        id: sumologic.endpoint.metrics.control.plane
        weight: 90
      default:
        tag: prometheus.metrics**
        id: sumologic.endpoint.metrics
        weight: 100
  events:
    enabled: true
    statefulset:
      nodeSelector: {}
      tolerations: {}
      resources:
        limits:
          memory: 512Mi
          cpu: 200m
        requests:
          memory: 256Mi
          cpu: 100m
    sourceCategory: ""
sumologic:
  setup:
    job:
      image:
        repository: sumologic/kubernetes-fluentd
        tag: 1.1.0
        pullPolicy: IfNotPresent
      annotations:
        helm.sh/hook: pre-install,pre-upgrade
        helm.sh/hook-weight: 3
        helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    clusterRole:
      annotations:
        helm.sh/hook: pre-install,pre-upgrade
        helm.sh/hook-weight: 1
        helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    clusterRoleBinding:
      annotations:
        helm.sh/hook: pre-install,pre-upgrade
        helm.sh/hook-weight: 2
        helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    configMap:
      annotations:
        helm.sh/hook: pre-install,pre-upgrade
        helm.sh/hook-weight: 2
        helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    serviceAccount:
      annotations:
        helm.sh/hook: pre-install,pre-upgrade
        helm.sh/hook-weight: 0
        helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
  setupEnabled: true
  endpoint: ""
  httpProxy: ""
  httpsProxy: ""
  clusterName: kubernetes
  cluster:
    host: https://kubernetes.default.svc
    cluster_ca_certificate: ${file("/var/run/secrets/kubernetes.io/serviceaccount/ca.crt")}
    token: ${file("/var/run/secrets/kubernetes.io/serviceaccount/token")}
    load_config_file: false
  collectionMonitoring: true
  logs:
    enabled: true
  metrics:
    enabled: false
  traces:
    enabled: false
    fluentd_stdout: false
    spans_per_request: 100
  collector:
    sources:
      metrics:
        default:
          name: (default-metrics)
          config-name: endpoint-metrics
        apiserver:
          name: apiserver-metrics
          config-name: endpoint-metrics-apiserver
        controller:
          name: kube-controller-manager-metrics
          config-name: endpoint-metrics-kube-controller-manager
        scheduler:
          name: kube-scheduler-metrics
          config-name: endpoint-metrics-kube-scheduler
        state:
          name: kube-state-metrics
          config-name: endpoint-metrics-kube-state
        kubelet:
          name: kubelet-metrics
          config-name: endpoint-metrics-kubelet
        node:
          name: node-exporter-metrics
          config-name: endpoint-metrics-node-exporter
        control-plane:
          name: control-plane-metrics
      logs:
        default:
          name: logs
          config-name: endpoint-logs
      events:
        default:
          name: events
          config-name: endpoint-events
          category: true
      traces:
        default:
          name: traces
          config-name: endpoint-traces
          properties:
            content_type: Zipkin
    fields: {}
nameOverride: ""
metrics-server:
  enabled: false
  extraArgs:
    kubelet-insecure-tls: true
    kubelet-preferred-address-types: InternalIP,ExternalIP,Hostname
fluent-bit:
  resources: {}
  service:
    labels:
      sumologic.com/scrape: "true"
  env:
    - name: FLUENTD_LOGS_SVC
      valueFrom:
        configMapKeyRef:
          name: sumologic-configmap
          key: fluentdLogs
    - name: NAMESPACE
      valueFrom:
        fieldRef:
          fieldPath: metadata.namespace
  tolerations:
    - effect: NoSchedule
      operator: Exists
  config:
    inputs: |-
      [INPUT]
          Name                  tail
          Path                  /var/log/containers/*.log
          Docker_Mode           On
          Docker_Mode_Parser    multi_line
          Tag                   containers.*
          Refresh_Interval      1
          Rotate_Wait           60
          Mem_Buf_Limit         5MB
          Skip_Long_Lines       On
          DB                    /tail-db/tail-containers-state-sumo.db
          DB.Sync               Normal
          Ignore_Older          24h
      [INPUT]
          Name                  systemd
          Tag                   host.*
          DB                    /tail-db/systemd-state-sumo.db
          Systemd_Filter        _SYSTEMD_UNIT=addon-config.service
          Systemd_Filter        _SYSTEMD_UNIT=addon-run.service
          Systemd_Filter        _SYSTEMD_UNIT=cfn-etcd-environment.service
          Systemd_Filter        _SYSTEMD_UNIT=cfn-signal.service
          Systemd_Filter        _SYSTEMD_UNIT=clean-ca-certificates.service
          Systemd_Filter        _SYSTEMD_UNIT=containerd.service
          Systemd_Filter        _SYSTEMD_UNIT=coreos-metadata.service
          Systemd_Filter        _SYSTEMD_UNIT=coreos-setup-environment.service
          Systemd_Filter        _SYSTEMD_UNIT=coreos-tmpfiles.service
          Systemd_Filter        _SYSTEMD_UNIT=dbus.service
          Systemd_Filter        _SYSTEMD_UNIT=docker.service
          Systemd_Filter        _SYSTEMD_UNIT=efs.service
          Systemd_Filter        _SYSTEMD_UNIT=etcd-member.service
          Systemd_Filter        _SYSTEMD_UNIT=etcd.service
          Systemd_Filter        _SYSTEMD_UNIT=etcd2.service
          Systemd_Filter        _SYSTEMD_UNIT=etcd3.service
          Systemd_Filter        _SYSTEMD_UNIT=etcdadm-check.service
          Systemd_Filter        _SYSTEMD_UNIT=etcdadm-reconfigure.service
          Systemd_Filter        _SYSTEMD_UNIT=etcdadm-save.service
          Systemd_Filter        _SYSTEMD_UNIT=etcdadm-update-status.service
          Systemd_Filter        _SYSTEMD_UNIT=flanneld.service
          Systemd_Filter        _SYSTEMD_UNIT=format-etcd2-volume.service
          Systemd_Filter        _SYSTEMD_UNIT=kube-node-taint-and-uncordon.service
          Systemd_Filter        _SYSTEMD_UNIT=kubelet.service
          Systemd_Filter        _SYSTEMD_UNIT=ldconfig.service
          Systemd_Filter        _SYSTEMD_UNIT=locksmithd.service
          Systemd_Filter        _SYSTEMD_UNIT=logrotate.service
          Systemd_Filter        _SYSTEMD_UNIT=lvm2-monitor.service
          Systemd_Filter        _SYSTEMD_UNIT=mdmon.service
          Systemd_Filter        _SYSTEMD_UNIT=nfs-idmapd.service
          Systemd_Filter        _SYSTEMD_UNIT=nfs-mountd.service
          Systemd_Filter        _SYSTEMD_UNIT=nfs-server.service
          Systemd_Filter        _SYSTEMD_UNIT=nfs-utils.service
          Systemd_Filter        _SYSTEMD_UNIT=node-problem-detector.service
          Systemd_Filter        _SYSTEMD_UNIT=ntp.service
          Systemd_Filter        _SYSTEMD_UNIT=oem-cloudinit.service
          Systemd_Filter        _SYSTEMD_UNIT=rkt-gc.service
          Systemd_Filter        _SYSTEMD_UNIT=rkt-metadata.service
          Systemd_Filter        _SYSTEMD_UNIT=rpc-idmapd.service
          Systemd_Filter        _SYSTEMD_UNIT=rpc-mountd.service
          Systemd_Filter        _SYSTEMD_UNIT=rpc-statd.service
          Systemd_Filter        _SYSTEMD_UNIT=rpcbind.service
          Systemd_Filter        _SYSTEMD_UNIT=set-aws-environment.service
          Systemd_Filter        _SYSTEMD_UNIT=system-cloudinit.service
          Systemd_Filter        _SYSTEMD_UNIT=systemd-timesyncd.service
          Systemd_Filter        _SYSTEMD_UNIT=update-ca-certificates.service
          Systemd_Filter        _SYSTEMD_UNIT=user-cloudinit.service
          Systemd_Filter        _SYSTEMD_UNIT=var-lib-etcd2.service
          Max_Entries           1000
          Read_From_Tail        true
    outputs: |-
      [OUTPUT]
          Name                  forward
          Match                 *
          Host                  ${FLUENTD_LOGS_SVC}.${NAMESPACE}.svc.cluster.local.
          Port                  24321
          tls                   off
          tls.verify            on
          tls.debug             1
    customParsers: |-
      [PARSER]
          Name                  multi_line
          Format                regex
          Regex                 (?<log>^{"log":"\d{4}-\d{1,2}-\d{1,2}.\d{2}:\d{2}:\d{2}.*)
kube-prometheus-stack:
  prometheusOperator:
    tls:
      enabled: false
    resources: {}
    admissionWebhooks:
      enabled: false
    kube-state-metrics:
      resources: {}
    prometheus-node-exporter:
      resources: {}
  kubeApiServer:
    serviceMonitor:
      interval: ""
  kubelet:
    serviceMonitor:
      interval: ""
  kubeControllerManager:
    serviceMonitor:
      interval: ""
  coreDns:
    serviceMonitor:
      interval: ""
  kubeEtcd:
    serviceMonitor:
      interval: ""
  kubeScheduler:
    serviceMonitor:
      interval: ""
  kubeStateMetrics:
    serviceMonitor:
      interval: ""
  nodeExporter:
    serviceMonitor:
      interval: ""
  alertmanager:
    enabled: false
  grafana:
    enabled: false
    defaultDashboardsEnabled: false
  prometheus:
    additionalServiceMonitors:
      - name: collection-sumologic-fluentd-logs
        additionalLabels:
          sumologic.com/app: fluentd-logs
        endpoints:
          - port: metrics
        namespaceSelector:
          matchNames:
            - $(NAMESPACE)
        selector:
          matchLabels:
            sumologic.com/app: fluentd-logs
            sumologic.com/scrape: "true"
      - name: collection-sumologic-fluentd-metrics
        additionalLabels:
          sumologic.com/app: fluentd-metrics
        endpoints:
          - port: metrics
        namespaceSelector:
          matchNames:
            - $(NAMESPACE)
        selector:
          matchLabels:
            sumologic.com/app: fluentd-metrics
            sumologic.com/scrape: "true"
      - name: collection-sumologic-fluentd-events
        additionalLabels:
          sumologic.com/app: fluentd-events
        endpoints:
          - port: metrics
        namespaceSelector:
          matchNames:
            - $(NAMESPACE)
        selector:
          matchLabels:
            sumologic.com/app: fluentd-events
            sumologic.com/scrape: "true"
      - name: collection-fluent-bit
        additionalLabels:
          app: collection-fluent-bit
        endpoints:
          - port: metrics
            path: /api/v1/metrics/prometheus
        namespaceSelector:
          matchNames:
            - $(NAMESPACE)
        selector:
          matchLabels:
            app: fluent-bit
            sumologic.com/scrape: "true"
      - name: collection-sumologic-otelcol
        additionalLabels:
          sumologic.com/app: otelcol
        endpoints:
          - port: metrics
        namespaceSelector:
          matchNames:
            - $(NAMESPACE)
        selector:
          matchLabels:
            sumologic.com/app: otelcol
            sumologic.com/scrape: "true"
    prometheusSpec:
      scrapeInterval: 30s
      resources: {}
      thanos:
        baseImage: quay.io/thanos/thanos
        version: v0.10.0
      containers:
        - name: config-reloader
          env:
            - name: FLUENTD_METRICS_SVC
              valueFrom:
                configMapKeyRef:
                  name: sumologic-configmap
                  key: fluentdMetrics
            - name: NAMESPACE
              valueFrom:
                configMapKeyRef:
                  name: sumologic-configmap
                  key: fluentdNamespace
      walCompression: true
      remoteWrite:
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local:9888/prometheus.metrics.control-plane.kube-etcd
          writeRelabelConfigs:
            - action: keep
              regex: kube-etcd;(?:etcd_debugging_(mvcc_db_total_size_in_bytes|store_(expires_total|watchers))|etcd_disk_(backend_commit|wal_fsync)_duration_seconds_bucket|etcd_grpc_proxy_cache_(hits|misses)_total|etcd_network_client_grpc_(received|sent)_bytes_total|etcd_server_(has_leader|leader_changes_seen_total)|etcd_server_proposals_(pending|(applied|committed|failed)_total)|process_(cpu_seconds_total|open_fds|resident_memory_bytes))
              sourceLabels:
                - job
                - __name__
          remoteTimeout: 5s
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local:9888/prometheus.metrics
          writeRelabelConfigs:
            - action: keep
              regex: (?:up|prometheus_remote_storage_.*|fluentd_.*|fluentbit.*|otelcol.*)
              sourceLabels:
                - __name__
          remoteTimeout: 5s
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local:9888/prometheus.metrics.node
          writeRelabelConfigs:
            - action: keep
              regex: node-exporter;(?:node_load1|node_load5|node_load15|node_cpu_seconds_total)
              sourceLabels:
                - job
                - __name__
          remoteTimeout: 5s
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local:9888/prometheus.metrics.container
          writeRelabelConfigs:
            - action: keep
              regex: kubelet;(?:container_network_receive_bytes_total|container_network_transmit_bytes_total)
              sourceLabels:
                - job
                - __name__
          remoteTimeout: 5s
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local:9888/prometheus.metrics.container
          writeRelabelConfigs:
            - action: labelmap
              regex: container_name
              replacement: container
            - action: drop
              regex: POD
              sourceLabels:
                - container
            - action: keep
              regex: kubelet;.+;(?:container_cpu_usage_seconds_total|container_memory_working_set_bytes|container_fs_usage_bytes|container_fs_limit_bytes)
              sourceLabels:
                - job
                - container
                - __name__
          remoteTimeout: 5s
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local:9888/prometheus.metrics.kubelet
          writeRelabelConfigs:
            - action: keep
              regex: kubelet;(?:kubelet_docker_operations_errors(?:|_total)|kubelet_(?:docker|runtime)_operations_duration_seconds_(?:count|sum)|kubelet_running_(?:container|pod)_count|kubelet_(:?docker|runtime)_operations_latency_microseconds(?:|_count|_sum))
              sourceLabels:
                - job
                - __name__
          remoteTimeout: 5s
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local:9888/prometheus.metrics.apiserver
          writeRelabelConfigs:
            - action: keep
              regex: apiserver;(?:apiserver_request_(?:count|total)|apiserver_request_(?:duration_seconds|latencies)_(?:count|sum)|apiserver_request_latencies_summary(?:|_count|_sum)|etcd_request_cache_(?:add|get)_(?:duration_seconds|latencies_summary)_(?:count|sum)|etcd_helper_cache_(?:hit|miss)_(?:count|total))
              sourceLabels:
                - job
                - __name__
          remoteTimeout: 5s
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local:9888/prometheus.metrics.scheduler
          writeRelabelConfigs:
            - action: keep
              regex: kube-scheduler;scheduler_(?:e2e_scheduling|binding|scheduling_algorithm)_duration_seconds.*
              sourceLabels:
                - job
                - __name__
          remoteTimeout: 5s
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local:9888/prometheus.metrics.controller-manager
          writeRelabelConfigs:
            - action: keep
              regex: kubelet;cloudprovider_.*_api_request_duration_seconds.*
              sourceLabels:
                - job
                - __name__
          remoteTimeout: 5s
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local:9888/prometheus.metrics.state
          writeRelabelConfigs:
            - action: keep
              regex: kube-state-metrics;(?:kube_statefulset_status_observed_generation|kube_statefulset_status_replicas|kube_statefulset_replicas|kube_statefulset_metadata_generation|kube_daemonset_status_current_number_scheduled|kube_daemonset_status_desired_number_scheduled|kube_daemonset_status_number_misscheduled|kube_daemonset_status_number_unavailable|kube_deployment_spec_replicas|kube_deployment_status_replicas_available|kube_deployment_status_replicas_unavailable|kube_node_info|kube_node_status_allocatable|kube_node_status_capacity|kube_node_status_condition|kube_pod_container_info|kube_pod_container_resource_requests|kube_pod_container_resource_limits|kube_pod_container_status_ready|kube_pod_container_status_terminated_reason|kube_pod_container_status_waiting_reason|kube_pod_container_status_restarts_total|kube_pod_status_phase)
              sourceLabels:
                - job
                - __name__
          remoteTimeout: 5s
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local:9888/prometheus.metrics.operator.rule
          remoteTimeout: 5s
          writeRelabelConfigs:
            - action: keep
              regex: 'cluster_quantile:apiserver_request_duration_seconds:histogram_quantile|instance:node_filesystem_usage:sum|instance:node_network_receive_bytes:rate:sum|cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile|cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile|cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile|node_namespace_pod:kube_pod_info:|:kube_pod_info_node_count:|node:node_num_cpu:sum|:node_cpu_utilisation:avg1m|node:node_cpu_utilisation:avg1m|node:cluster_cpu_utilisation:ratio|:node_cpu_saturation_load1:|node:node_cpu_saturation_load1:|:node_memory_utilisation:|node:node_memory_bytes_total:sum|node:node_memory_utilisation:ratio|node:cluster_memory_utilisation:ratio|:node_memory_swap_io_bytes:sum_rate|node:node_memory_utilisation:|node:node_memory_utilisation_2:|node:node_memory_swap_io_bytes:sum_rate|:node_disk_utilisation:avg_irate|node:node_disk_utilisation:avg_irate|:node_disk_saturation:avg_irate|node:node_disk_saturation:avg_irate|node:node_filesystem_usage:|node:node_filesystem_avail:|:node_net_utilisation:sum_irate|node:node_net_utilisation:sum_irate|:node_net_saturation:sum_irate|node:node_net_saturation:sum_irate|node:node_inodes_total:|node:node_inodes_free:'
              sourceLabels: [__name__]
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local:9888/prometheus.metrics.control-plane.coredns
          remoteTimeout: 5s
          writeRelabelConfigs:
            - action: keep
              regex: 'coredns;(?:coredns_cache_(size|entries|(hits|misses)_total)|coredns_dns_request_duration_seconds_(count|sum)|coredns_(dns_request|dns_response_rcode|forward_request)_count_total|coredns_(forward_requests|dns_requests|dns_responses)_total|process_(cpu_seconds_total|open_fds|resident_memory_bytes))'
              sourceLabels: [job, __name__]
  additionalPrometheusRulesMap:
    pre-1.14-node-rules:
      groups:
        - name: node-pre-1.14.rules
          rules:
            - expr: 1 - avg(rate(node_cpu_seconds_total{job="node-exporter",mode="idle"}[1m]))
              record: :node_cpu_utilisation:avg1m
            - expr: |-
                1 - avg by (node) (
                  rate(node_cpu_seconds_total{job="node-exporter",mode="idle"}[1m])
                * on (namespace, pod) group_left(node)
                  node_namespace_pod:kube_pod_info:)
              record: node:node_cpu_utilisation:avg1m
            - expr: |-
                1 -
                sum(
                  node_memory_MemFree_bytes{job="node-exporter"} +
                  node_memory_Cached_bytes{job="node-exporter"} +
                  node_memory_Buffers_bytes{job="node-exporter"}
                )
                /
                sum(node_memory_MemTotal_bytes{job="node-exporter"})
              record: ':node_memory_utilisation:'
            - expr: |-
                sum by (node) (
                  (
                    node_memory_MemFree_bytes{job="node-exporter"} +
                    node_memory_Cached_bytes{job="node-exporter"} +
                    node_memory_Buffers_bytes{job="node-exporter"}
                  )
                  * on (namespace, pod) group_left(node)
                    node_namespace_pod:kube_pod_info:
                )
              record: node:node_memory_bytes_available:sum
            - expr: |-
                (node:node_memory_bytes_total:sum - node:node_memory_bytes_available:sum)
                /
                node:node_memory_bytes_total:sum
              record: node:node_memory_utilisation:ratio
            - expr: |-
                1 -
                sum by (node) (
                  (
                    node_memory_MemFree_bytes{job="node-exporter"} +
                    node_memory_Cached_bytes{job="node-exporter"} +
                    node_memory_Buffers_bytes{job="node-exporter"}
                  )
                * on (namespace, pod) group_left(node)
                  node_namespace_pod:kube_pod_info:
                )
                /
                sum by (node) (
                  node_memory_MemTotal_bytes{job="node-exporter"}
                * on (namespace, pod) group_left(node)
                  node_namespace_pod:kube_pod_info:
                )
              record: 'node:node_memory_utilisation:'
            - expr: 1 - (node:node_memory_bytes_available:sum / node:node_memory_bytes_total:sum)
              record: 'node:node_memory_utilisation_2:'
            - expr: |-
                max by (instance, namespace, pod, device) ((node_filesystem_size_bytes{fstype=~"ext[234]|btrfs|xfs|zfs"}
                - node_filesystem_avail_bytes{fstype=~"ext[234]|btrfs|xfs|zfs"})
                / node_filesystem_size_bytes{fstype=~"ext[234]|btrfs|xfs|zfs"})
              record: 'node:node_filesystem_usage:'
            - expr: |-
                sum by (node) (
                  node_memory_MemTotal_bytes{job="node-exporter"}
                  * on (namespace, pod) group_left(node)
                    node_namespace_pod:kube_pod_info:
                )
              record: node:node_memory_bytes_total:sum
            - expr: |-
                sum(irate(node_network_receive_bytes_total{job="node-exporter",device!~"veth.+"}[1m])) +
                sum(irate(node_network_transmit_bytes_total{job="node-exporter",device!~"veth.+"}[1m]))
              record: :node_net_utilisation:sum_irate
            - expr: |-
                sum by (node) (
                  (irate(node_network_receive_bytes_total{job="node-exporter",device!~"veth.+"}[1m]) +
                  irate(node_network_transmit_bytes_total{job="node-exporter",device!~"veth.+"}[1m]))
                * on (namespace, pod) group_left(node)
                  node_namespace_pod:kube_pod_info:
                )
              record: node:node_net_utilisation:sum_irate
            - expr: |-
                sum(irate(node_network_receive_drop_total{job="node-exporter",device!~"veth.+"}[1m])) +
                sum(irate(node_network_transmit_drop_total{job="node-exporter",device!~"veth.+"}[1m]))
              record: :node_net_saturation:sum_irate
            - expr: |-
                sum by (node) (
                  (irate(node_network_receive_drop_total{job="node-exporter",device!~"veth.+"}[1m]) +
                  irate(node_network_transmit_drop_total{job="node-exporter",device!~"veth.+"}[1m]))
                * on (namespace, pod) group_left(node)
                  node_namespace_pod:kube_pod_info:
                )
              record: node:node_net_saturation:sum_irate
            - expr: |-
                max by (instance, namespace, pod, device) ((node_filesystem_size_bytes{fstype=~"ext[234]|btrfs|xfs|zfs"}
                - node_filesystem_avail_bytes{fstype=~"ext[234]|btrfs|xfs|zfs"})
                / node_filesystem_size_bytes{fstype=~"ext[234]|btrfs|xfs|zfs"})
              record: 'node:node_filesystem_usage:'
            - expr: |-
                sum(node_load1{job="node-exporter"})
                /
                sum(node:node_num_cpu:sum)
              record: ':node_cpu_saturation_load1:'
            - expr: |-
                sum by (node) (
                  node_load1{job="node-exporter"}
                * on (namespace, pod) group_left(node)
                  node_namespace_pod:kube_pod_info:
                )
                /
                node:node_num_cpu:sum
              record: 'node:node_cpu_saturation_load1:'
            - expr: avg(irate(node_disk_io_time_weighted_seconds_total{job="node-exporter",device=~"nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+"}[1m]))
              record: :node_disk_saturation:avg_irate
            - expr: |-
                avg by (node) (
                  irate(node_disk_io_time_weighted_seconds_total{job="node-exporter",device=~"nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+"}[1m])
                * on (namespace, pod) group_left(node)
                  node_namespace_pod:kube_pod_info:
                )
              record: node:node_disk_saturation:avg_irate
            - expr: avg(irate(node_disk_io_time_seconds_total{job="node-exporter",device=~"nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+"}[1m]))
              record: :node_disk_utilisation:avg_irate
            - expr: |-
                avg by (node) (
                  irate(node_disk_io_time_seconds_total{job="node-exporter",device=~"nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+"}[1m])
                * on (namespace, pod) group_left(node)
                  node_namespace_pod:kube_pod_info:
                )
              record: node:node_disk_utilisation:avg_irate
            - expr: |-
                1e3 * sum(
                  (rate(node_vmstat_pgpgin{job="node-exporter"}[1m])
                + rate(node_vmstat_pgpgout{job="node-exporter"}[1m]))
                )
              record: :node_memory_swap_io_bytes:sum_rate
            - expr: |-
                1e3 * sum by (node) (
                  (rate(node_vmstat_pgpgin{job="node-exporter"}[1m])
                + rate(node_vmstat_pgpgout{job="node-exporter"}[1m]))
                * on (namespace, pod) group_left(node)
                  node_namespace_pod:kube_pod_info:
                )
              record: node:node_memory_swap_io_bytes:sum_rate
            - expr: |-
                node:node_cpu_utilisation:avg1m
                  *
                node:node_num_cpu:sum
                  /
                scalar(sum(node:node_num_cpu:sum))
              record: node:cluster_cpu_utilisation:ratio
            - expr: |-
                (node:node_memory_bytes_total:sum - node:node_memory_bytes_available:sum)
                /
                scalar(sum(node:node_memory_bytes_total:sum))
              record: node:cluster_memory_utilisation:ratio
            - expr: |-
                sum by (node) (
                  node_load1{job="node-exporter"}
                * on (namespace, pod) group_left(node)
                  node_namespace_pod:kube_pod_info:
                )
                /
                node:node_num_cpu:sum
              record: 'node:node_cpu_saturation_load1:'
            - expr: |-
                max by (instance, namespace, pod, device) (
                  node_filesystem_avail_bytes{fstype=~"ext[234]|btrfs|xfs|zfs"}
                  /
                  node_filesystem_size_bytes{fstype=~"ext[234]|btrfs|xfs|zfs"}
                  )
              record: 'node:node_filesystem_avail:'
            - expr: |-
                max by (instance, namespace, pod, device) ((node_filesystem_size_bytes{fstype=~"ext[234]|btrfs|xfs|zfs"}
                - node_filesystem_avail_bytes{fstype=~"ext[234]|btrfs|xfs|zfs"})
                / node_filesystem_size_bytes{fstype=~"ext[234]|btrfs|xfs|zfs"})
              record: 'node:node_filesystem_usage:'
            - expr: |-
                max(
                  max(
                    kube_pod_info{job="kube-state-metrics", host_ip!=""}
                  ) by (node, host_ip)
                  * on (host_ip) group_right (node)
                  label_replace(
                    (
                      max(node_filesystem_files{job="node-exporter", mountpoint="/"})
                      by (instance)
                    ), "host_ip", "$1", "instance", "(.*):.*"
                  )
                ) by (node)
              record: 'node:node_inodes_total:'
            - expr: |-
                max(
                  max(
                    kube_pod_info{job="kube-state-metrics", host_ip!=""}
                  ) by (node, host_ip)
                  * on (host_ip) group_right (node)
                  label_replace(
                    (
                      max(node_filesystem_files_free{job="node-exporter", mountpoint="/"})
                      by (instance)
                    ), "host_ip", "$1", "instance", "(.*):.*"
                  )
                ) by (node)
              record: 'node:node_inodes_free:'
falco:
  enabled: false
  falco:
    jsonOutput: true
otelcol:
  deployment:
    nodeSelector: {}
    tolerations: {}
    replicas: 1
    resources:
      limits:
        memory: 2Gi
        cpu: 1
      requests:
        memory: 384Mi
        cpu: 200m
    memBallastSizeMib: 683
    image:
      repository: sumologic/opentelemetry-collector
      tag: 0.4.0.0
      pullPolicy: IfNotPresent
  config:
    receivers:
      jaeger:
        protocols:
          thrift_compact:
            endpoint: 0.0.0.0:6831
          thrift_binary:
            endpoint: 0.0.0.0:6832
          grpc:
            endpoint: 0.0.0.0:14250
          thrift_http:
            endpoint: 0.0.0.0:14268
      opencensus:
        endpoint: 0.0.0.0:55678
      otlp:
        endpoint: 0.0.0.0:55680
      zipkin:
        endpoint: 0.0.0.0:9411
    processors:
      k8s_tagger:
        passthrough: false
        owner_lookup_enabled: true
        extract:
          metadata:
            - containerId
            - containerName
            - clusterName
            - daemonSetName
            - deploymentName
            - hostName
            - namespace
            - nodeName
            - podId
            - podName
            - replicaSetName
            - serviceName
            - statefulSetName
          annotations:
            - tag_name: k8s.pod.annotation.%s
              key: '*'
          namespace_labels:
            - tag_name: k8s.namespace.label.%s
              key: '*'
          labels:
            - tag_name: k8s.pod.label.%s
              key: '*'
      source:
        collector: processors.source.collector.replace
        source_name: processors.source.name.replace
        source_category: processors.source.category.replace
        source_category_prefix: processors.source.category_prefix.replace
        source_category_replace_dash: processors.source.category_replace_dash.replace
        exclude_namespace_regex: processors.source.exclude_namespace_regex.replace
        exclude_pod_regex: processors.source.exclude_pod_regex.replace
        exclude_container_regex: processors.source.exclude_container_regex.replace
        exclude_host_regex: processors.source.exclude_host_regex.replace
        annotation_prefix: k8s.pod.annotation.
        pod_template_hash_key: k8s.pod.label.pod-template-hash
        pod_name_key: k8s.pod.pod_name
        namespace_key: k8s.namespace.name
        pod_key: k8s.pod.name
        container_key: k8s.container.name
        source_host_key: k8s.pod.hostname
      resource:
        labels:
          k8s.cluster.name: processors.resource.cluster.replace
      memory_limiter:
        check_interval: 5s
        limit_mib: 1900
      queued_retry:
        num_workers: 16
        queue_size: 10000
        retry_on_failure: true
      batch:
        send_batch_size: 256
        send_batch_hard_limit: 512
        timeout: 5s
    extensions:
      health_check: {}
    exporters:
      zipkin:
        url: ${SUMO_ENDPOINT_DEFAULT_TRACES_SOURCE}
    service:
      extensions:
        - health_check
      pipelines:
        traces:
          receivers:
            - jaeger
            - opencensus
            - otlp
            - zipkin
          processors:
            - memory_limiter
            - k8s_tagger
            - source
            - resource
            - batch
            - queued_retry
          exporters:
            - zipkin
